{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-core 1.52.0 requires packaging<=23.0,>=20.0, but you have packaging 23.2 which is incompatible.\n",
      "docker 6.1.3 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\n",
      "jupyterlab 4.0.2 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "jupyterlab-server 2.23.0 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "jupyterlab-server 2.23.0 requires requests>=2.28, but you have requests 2.25.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain) (6.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.30-cp39-cp39-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.57-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain) (1.26.1)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ------------------------------------ 107.3/107.3 kB 326.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain) (2.25.1)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.3-cp39-none-win_amd64.whl.metadata (50 kB)\n",
      "     -------------------------------------- 50.9/50.9 kB 521.7 kB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.2-cp39-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain) (2021.5.30)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 302.9/302.9 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
      "   -------------------------------------- 121.0/121.0 kB 710.1 kB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 409.3/409.3 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp39-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.30-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "   ---------------------------------------- 290.8/290.8 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 49.3/49.3 kB 279.0 kB/s eta 0:00:00\n",
      "Downloading orjson-3.10.3-cp39-none-win_amd64.whl (138 kB)\n",
      "   -------------------------------------- 138.6/138.6 kB 910.4 kB/s eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 53.0/53.0 kB 195.3 kB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, pydantic-core, packaging, orjson, mypy-extensions, jsonpatch, greenlet, annotated-types, typing-inspect, SQLAlchemy, pydantic, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "Successfully installed SQLAlchemy-2.0.30 annotated-types-0.6.0 dataclasses-json-0.6.6 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.57 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 tenacity-8.3.0 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.13.7-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting libmagic\n",
      "  Downloading libmagic-1.0.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting python-magic-bin\n",
      "  Downloading python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl.metadata (710 bytes)\n",
      "Requirement already satisfied: chardet in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.0.0)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (5.1.0)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (0.6.6)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     -------------------------------------- 981.5/981.5 kB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (1.26.1)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.9.0-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.11.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.22.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from unstructured) (1.12.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json->unstructured) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from langdetect->unstructured) (1.15.0)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (4.66.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (2021.5.30)\n",
      "Collecting certifi>=2017.4.17 (from requests->unstructured)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer>=3.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
      "  Downloading deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "INFO: pip is looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.21.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading unstructured_client-0.21.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Downloading unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client->unstructured)\n",
      "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.17.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.16.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.15.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.15.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "INFO: pip is still looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading unstructured_client-0.15.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.15.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.14.3-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading unstructured_client-0.12.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading unstructured_client-0.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Downloading unstructured_client-0.8.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.1.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (3.1.0)\n",
      "  Downloading unstructured_client-0.8.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading unstructured_client-0.7.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Downloading unstructured_client-0.7.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Downloading unstructured_client-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Downloading unstructured_client-0.7.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Downloading unstructured_client-0.6.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "  Downloading unstructured_client-0.5.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Downloading unstructured_client-0.5.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "  Downloading unstructured_client-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading unstructured_client-0.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting requests (from unstructured)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->unstructured)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (23.2)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Collecting six (from langdetect->unstructured)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->unstructured)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk->unstructured) (0.4.4)\n",
      "Downloading unstructured-0.13.7-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl (409 kB)\n",
      "   ---------------------------------------- 409.3/409.3 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 433.8/433.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
      "   ---------------------------------------- 274.7/274.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.9.0-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 62.6/62.6 kB 556.7 kB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 66.8/66.8 kB 602.1 kB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 163.8/163.8 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "   -------------------------------------- 100.4/100.4 kB 958.9 kB/s eta 0:00:00\n",
      "Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 80.8/80.8 kB 410.4 kB/s eta 0:00:00\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 290.4/290.4 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 121.1/121.1 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 97.9/97.9 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: libmagic, langdetect\n",
      "  Building wheel for libmagic (setup.py): started\n",
      "  Building wheel for libmagic (setup.py): finished with status 'done'\n",
      "  Created wheel for libmagic: filename=libmagic-1.0-py3-none-any.whl size=4298 sha256=7cd83e57a9c6d36d9d4023a7a0b6790d383c3491831d6d2ebe93faca4b4221c1\n",
      "  Stored in directory: c:\\users\\hammaad\\appdata\\local\\pip\\cache\\wheels\\f8\\8a\\32\\b5e5458427778faf24ad32526c49e2dc5d20f32c96bf6ce236\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=b04549b9b310d1b7edd1c8abd1c76c55a1f60424b6773545e79a570c0fd8b325\n",
      "  Stored in directory: c:\\users\\hammaad\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built libmagic langdetect\n",
      "Installing collected packages: python-magic-bin, libmagic, filetype, urllib3, six, rapidfuzz, python-magic, python-iso639, pypdf, ordered-set, jsonpath-python, idna, emoji, click, charset-normalizer, certifi, backoff, requests, nltk, langdetect, deepdiff, unstructured-client, unstructured\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.1.0\n",
      "    Uninstalling charset-normalizer-3.1.0:\n",
      "      Successfully uninstalled charset-normalizer-3.1.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.5.30\n",
      "    Uninstalling certifi-2021.5.30:\n",
      "      Successfully uninstalled certifi-2021.5.30\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "Successfully installed backoff-2.2.1 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 deepdiff-7.0.1 emoji-2.11.1 filetype-1.2.0 idna-3.7 jsonpath-python-1.0.6 langdetect-1.0.9 libmagic-1.0 nltk-3.8.1 ordered-set-4.1.0 pypdf-4.2.0 python-iso639-2024.4.27 python-magic-0.4.27 python-magic-bin-0.4.14 rapidfuzz-3.9.0 requests-2.31.0 six-1.16.0 unstructured-0.13.7 unstructured-client-0.22.0 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-core 1.52.0 requires packaging<=23.0,>=20.0, but you have packaging 23.2 which is incompatible.\n",
      "azureml-core 1.52.0 requires urllib3<2.0.0,>=1.23, but you have urllib3 2.2.1 which is incompatible.\n",
      "azureml-dataset-runtime 1.52.0 requires numpy!=1.19.4,<1.24; sys_platform == \"win32\", but you have numpy 1.26.1 which is incompatible.\n",
      "botocore 1.34.39 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "jupyterlab 4.0.2 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "jupyterlab-server 2.23.0 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "pyppeteer 1.0.2 requires urllib3<2.0.0,>=1.25.8, but you have urllib3 2.2.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Using cached unstructured-0.13.7-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting libmagic\n",
      "  Using cached libmagic-1.0-py3-none-any.whl\n",
      "Collecting python-magic\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting python-magic-bin\n",
      "  Using cached python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl.metadata (710 bytes)\n",
      "Requirement already satisfied: chardet in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.0.0)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (5.1.0)\n",
      "Collecting nltk (from unstructured)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for six: [Errno 2] No such file or directory: 'c:\\\\users\\\\hammaad\\\\appdata\\\\roaming\\\\python\\\\python39\\\\site-packages\\\\six-1.15.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for idna: [Errno 2] No such file or directory: 'c:\\\\users\\\\hammaad\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\idna-2.10.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for urllib3: [Errno 2] No such file or directory: 'c:\\\\users\\\\hammaad\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\urllib3-1.26.6.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-core 1.52.0 requires packaging<=23.0,>=20.0, but you have packaging 23.2 which is incompatible.\n",
      "azureml-core 1.52.0 requires urllib3<2.0.0,>=1.23, but you have urllib3 2.2.1 which is incompatible.\n",
      "azureml-dataset-runtime 1.52.0 requires numpy!=1.19.4,<1.24; sys_platform == \"win32\", but you have numpy 1.26.1 which is incompatible.\n",
      "botocore 1.34.39 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "jupyterlab 4.0.2 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "jupyterlab-server 2.23.0 requires jinja2>=3.0.3, but you have jinja2 3.0.1 which is incompatible.\n",
      "pyppeteer 1.0.2 requires urllib3<2.0.0,>=1.25.8, but you have urllib3 2.2.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting emoji (from unstructured)\n",
      "  Using cached emoji-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (0.6.6)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Using cached python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (1.26.1)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Using cached rapidfuzz-3.9.0-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured) (4.11.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Using cached unstructured_client-0.22.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from unstructured) (1.12.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json->unstructured) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from langdetect->unstructured) (1.15.0)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->unstructured) (4.66.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->unstructured) (2021.5.30)\n",
      "Collecting certifi>=2017.4.17 (from requests->unstructured)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer>=3.2.0 (from unstructured-client->unstructured)\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
      "  Using cached deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "INFO: pip is looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Using cached unstructured_client-0.21.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Using cached unstructured_client-0.21.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Using cached unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client->unstructured)\n",
      "  Using cached dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Using cached unstructured_client-0.17.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.16.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.15.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.15.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "INFO: pip is still looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached unstructured_client-0.15.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.15.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.14.3-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached unstructured_client-0.12.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached unstructured_client-0.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Using cached unstructured_client-0.8.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.1.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (3.1.0)\n",
      "  Using cached unstructured_client-0.8.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Using cached unstructured_client-0.7.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached unstructured_client-0.7.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached unstructured_client-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached unstructured_client-0.7.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached unstructured_client-0.6.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "  Using cached unstructured_client-0.5.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached unstructured_client-0.5.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "  Using cached unstructured_client-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached unstructured_client-0.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting requests (from unstructured)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->unstructured)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (23.2)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Collecting six (from langdetect->unstructured)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->unstructured)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
      "  Using cached ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk->unstructured) (0.4.4)\n",
      "Using cached unstructured-0.13.7-py3-none-any.whl (1.9 MB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl (409 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
      "Using cached rapidfuzz-3.9.0-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Using cached deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: python-magic-bin, libmagic, filetype, urllib3, six, rapidfuzz, python-magic, python-iso639, pypdf, ordered-set, jsonpath-python, idna, emoji, click, charset-normalizer, certifi, backoff, requests, nltk, langdetect, deepdiff, unstructured-client, unstructured\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Can't uninstall 'urllib3'. No files were found to uninstall.\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Can't uninstall 'six'. No files were found to uninstall.\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Can't uninstall 'idna'. No files were found to uninstall.\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.1.0\n",
      "    Can't uninstall 'charset-normalizer'. No files were found to uninstall.\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.5.30\n",
      "    Can't uninstall 'certifi'. No files were found to uninstall.\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed backoff-2.2.1 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 deepdiff-7.0.1 emoji-2.11.1 filetype-1.2.0 idna-3.7 jsonpath-python-1.0.6 langdetect-1.0.9 libmagic-1.0 nltk-3.8.1 ordered-set-4.1.0 pypdf-4.2.0 python-iso639-2024.4.27 python-magic-0.4.27 python-magic-bin-0.4.14 rapidfuzz-3.9.0 requests-2.31.0 six-1.16.0 unstructured-0.13.7 unstructured-client-0.22.0 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured libmagic python-magic python-magic-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured (CSV, txt) vs Unstructured Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"The stock of NVIDIA Corp (NASDAQ:NVDA) experienced a daily loss of -3.56% and a 3-month gain of 32.35%. With an Earnings Per Share (EPS) (EPS) of $1.92, the question arises: is the stock significantly overvalued? This article aims to provide a detailed valuation analysis of NVIDIA, offering insights into its financial strength, profitability, growth, and more. We invite you to delve into this comprehensive analysis.\\n\\nCompany Overview\\nWarning! GuruFocus has detected 10 Warning Signs with NVDA. Click here to check it out.\\n\\nNVDA 30-Year Financial Data\\n\\nThe intrinsic value of NVDA\\n\\n\\nNVIDIA Corp (NASDAQ:NVDA) is a leading designer of discrete graphics processing units that enhance the experience on computing platforms. The firm's chips are widely used in various end markets, including PC gaming and data centers. In recent years, NVIDIA has broadened its focus from traditional PC graphics applications such as gaming to more complex and favorable opportunities, including artificial intelligence and autonomous driving, leveraging the high-performance capabilities of its products.\\n\\nCurrently, NVIDIA's stock price stands at $418.01, significantly higher than the GF Value of $310.28, indicating the stock might be overvalued. With a market cap of $1 trillion, the valuation seems steep. The following analysis aims to delve deeper into the company's value.\\n\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nUnderstanding the GF Value\\nThe GF Value is a unique measure of the intrinsic value of a stock, calculated based on historical trading multiples, a GuruFocus adjustment factor, and future business performance estimates. If the stock price is significantly above the GF Value Line, it is overvalued, and its future return is likely to be poor. Conversely, if it is significantly below the GF Value Line, its future return will likely be higher.\\n\\nAccording to GuruFocus Value calculation, NVIDIA (NASDAQ:NVDA) appears to be significantly overvalued. The stock's current price of $418.01 per share and the market cap of $1 trillion further strengthen this assumption.\\n\\nGiven that NVIDIA is significantly overvalued, the long-term return of its stock is likely to be much lower than its future business growth.\\n\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nLink: These companies may deliver higher future returns at reduced risk.\\n\\nFinancial Strength of NVIDIA\\nExamining the financial strength of a company is crucial before investing in its stock. Companies with poor financial strength pose a higher risk of permanent loss. NVIDIA's cash-to-debt ratio of 1.27 is worse than 58.04% of companies in the Semiconductors industry. However, NVIDIA's overall financial strength is 8 out of 10, indicating a strong financial position.\\n\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nProfitability and Growth\\nConsistent profitability over the long term reduces the risk for investors. NVIDIA, with its profitability ranking of 10 out of 10, has been profitable for the past 10 years. The company's operating margin of 17.37% ranks better than 76.5% of companies in the Semiconductors industry.\\n\\nHowever, growth is a crucial factor in a company's valuation. NVIDIA's growth ranks worse than 52.99% of companies in the Semiconductors industry, with its 3-year average revenue growth rate better than 87.88% of companies in the industry.\\n\\nROIC vs WACC\\nComparing a company's return on invested capital (ROIC) to its weighted average cost of capital (WACC) is an effective way to evaluate its profitability. Over the past 12 months, NVIDIA's ROIC was 20.32 while its WACC was 16.74, suggesting that the company is creating value for its shareholders.\\n\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\nConclusion\\nIn conclusion, NVIDIA (NASDAQ:NVDA) appears to be significantly overvalued. Despite its strong financial condition and profitability, its growth ranks lower than 52.99% of companies in the Semiconductors industry. To learn more about NVIDIA stock, you can check out its 30-Year Financials here.\\n\\nTo find out the high quality companies that may deliver above-average returns, please check out GuruFocus High Quality Low Capex Screener.\\n\\nThis article first appeared on GuruFocus.\", metadata={'source': 'nvda_news_1.txt'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"nvda_news_1.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structured loader\n",
    "\n",
    "loader = CSVLoader(\"data.csv\",souce_column=\"column_name\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unstructured loader- News articles and etc\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "        \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "    ]\n",
    ")\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \n",
    "Kip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \n",
    "It has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 358, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CharacterTextSplitter doesnt perform well as it only splits based on one condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 200,  # size of each chunk created\n",
    "    chunk_overlap  = 0,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter combines individiaul character splitters, where if the chink exceeds the chunk size mentioned, it will be split based on its next seperator till the chunk is size is lesser than the chunk size needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vector Database: FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from faiss-cpu) (1.26.1)\n",
      "Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 14.5/14.5 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.26.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.0.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hammaad\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.10)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "   -------------------------------------- 171.5/171.5 kB 937.4 kB/s eta 0:00:00\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hammaad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df = pd.read_csv(\"sample_text.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence transformer is used to encode the sentence to numeric format, this is a package avaialbe from huggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdfbea6122a4409bd8642934a3b1b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hammaad\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128f92ddb506417fa49b2b25852fca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd25fa3b4c214871b472adb05c69e261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5190bbbfe4a94d359a2686b9f4e2c97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53153b7869f42c7849a6a6dcd1ec7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ede0ff3c72a4668b2944b906f7acc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4c8b918a0455c99b15b7d232699ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879b8cfea8494f409ed209d1bb22e1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2950efacff9418fb80bd4375427616d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cc90e9cf9a47c1bef414e9ec6d0925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f480d6ab633b4d3a908f9e83ee79c070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectors = encoder.encode(df.text)\n",
    "vectors.shape\n",
    "dim = vectors.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Faiss index adds the vectors we can now run new input based on these encodings and search for a simillar sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vibrant color jeans for male are becoming a trend</td>\n",
       "      <td>Fashion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category\n",
       "3  Vibrant color jeans for male are becoming a trend  Fashion"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query = \"I want to buy a polo t-shirt\"\n",
    "vec = encoder.encode(search_query)\n",
    "\n",
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "\n",
    "distances, I = index.search(svec, k=2)#no of simllar results\n",
    "distances\n",
    "row_indices = I.tolist()[0]\n",
    "row_indices\n",
    "df.loc[row_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we search something like a tshirt, now the econder knows its soemthing related to the domain fashion and returns items simillar to that domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exciting vacation destinations for your next trip</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maldives and Srilanka are gaining popularity in terms of low budget vacation places</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text  \\\n",
       "6                                    Exciting vacation destinations for your next trip   \n",
       "7  Maldives and Srilanka are gaining popularity in terms of low budget vacation places   \n",
       "\n",
       "  category  \n",
       "6   Travel  \n",
       "7   Travel  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query = \"I want to buy a Lamborghini\"\n",
    "vec = encoder.encode(search_query)\n",
    "\n",
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "\n",
    "distances, I = index.search(svec, k=2)#no of simllar results\n",
    "distances\n",
    "row_indices = I.tolist()[0]\n",
    "row_indices\n",
    "df.loc[row_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the FAISS algortihm is able to find the distance between simillar topics to the search query and return the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fruits, whole grains and vegetables helps control blood pressure</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meditation and yoga can improve mental health</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text category\n",
       "1  Fruits, whole grains and vegetables helps control blood pressure   Health\n",
       "0                     Meditation and yoga can improve mental health   Health"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query = \"An apple a day keeps the doctor away\"\n",
    "vec = encoder.encode(search_query)\n",
    "\n",
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "\n",
    "distances, I = index.search(svec, k=2)#no of simllar results\n",
    "distances\n",
    "row_indices = I.tolist()[0]\n",
    "row_indices\n",
    "df.loc[row_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining these individual components for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-OEUv6IKukVGXwybU4SXGT3BlbkFJMf7kuoR9g7OYKn0yhAK5'\n",
    "llm = OpenAI(temperature=0.9, max_tokens=500) # Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders = UnstructuredURLLoader(urls=[\n",
    "    \"https://www.moneycontrol.com/news/business/markets/wall-street-rises-as-tesla-soars-on-ai-optimism-11351111.html\",\n",
    "    \"https://www.moneycontrol.com/news/business/tata-motors-launches-punch-icng-price-starts-at-rs-7-1-lakh-11098751.html\"\n",
    "])\n",
    "data = loaders.load() \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6616/1318649053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Pass the documents and embeddings inorder to create FAISS vector index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvectorindex_openai\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \"\"\"\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m         return cls.__from(\n\u001b[0;32m    932\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     async def aembed_documents(\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             response = embed_with_retry(\n\u001b[0m\u001b[0;32m    495\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;34m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mretry_decorator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\resources\\embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;34m\"/embeddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m         )\n\u001b[1;32m-> 1240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m     def patch(\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m--> 921\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m                 return self._retry_request(\n\u001b[0m\u001b[0;32m   1006\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m   1054\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m                 return self._retry_request(\n\u001b[0m\u001b[0;32m   1006\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m   1054\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Hammaad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Re-raising status error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m         return self._process_response(\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Pass the documents and embeddings inorder to create FAISS vector index\n",
    "vectorindex_openai = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"vector_index.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(vectorindex_openai, f)\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        vectorIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorIndex.as_retriever())\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the price of Tiago iCNG?\"\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
